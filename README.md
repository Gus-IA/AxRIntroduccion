# Aprendizaje por Refuerzo en Tic-Tac-Toe (Juego del Gato)

Este proyecto implementa un agente de **aprendizaje por refuerzo** que aprende a jugar *Tic-Tac-Toe* mediante **self-play**.  
El modelo utiliza una tabla de valores (*value function*) donde asigna un valor a cada estado del tablero y se actualiza al finalizar cada partida.

---

## ğŸš€ Â¿QuÃ© hace este proyecto?

- Simula miles de partidas entre dos agentes.
- Los agentes aprenden mediante **exploraciÃ³n y explotaciÃ³n**.
- Guarda la **funciÃ³n de valor aprendida** en un archivo `agente.pickle`.
- Muestra en un DataFrame los estados mejor valorados por el agente.

---

## ğŸ§  Conceptos aprendidos

### ğŸ”¹ 1. RepresentaciÃ³n del Tablero
El tablero se maneja con una matriz `3x3` de NumPy, donde:
- `1` â†’ Jugador 1  
- `-1` â†’ Jugador 2  
- `0` â†’ Celda vacÃ­a  

Se implementan funciones para:
- Obtener movimientos vÃ¡lidos
- Actualizar el tablero
- Detectar victoria, empate o si el juego continÃºa

---

### ğŸ”¹ 2. Self-play
El agente juega contra otro agente (uno mÃ¡s explorador, uno mÃ¡s explotador).  
Usa mÃºltiples rondas para aprender estrategias ganadoras.

Cada ronda:
1. Ambos agentes juegan hasta terminar la partida.
2. Se les asignan recompensas:
   - Victoria â†’ `1`
   - Empate â†’ `0.5`
   - Derrota â†’ `0`

---

### ğŸ”¹ 3. PolÃ­tica de AcciÃ³n
El agente decide entre:
- **Explorar**: moverse aleatoriamente con probabilidad `prob_exp`.
- **Explotar**: elegir el movimiento con mayor valor estimado en la funciÃ³n de valor.

---

### ğŸ”¹ 4. ActualizaciÃ³n de la FunciÃ³n de Valor
Al final de cada partida, el agente actualiza sus valores con:


ğŸ§© Requisitos

Antes de ejecutar el script, instala las dependencias:

pip install -r requirements.txt

ğŸ§‘â€ğŸ’» Autor

Desarrollado por Gus como parte de su aprendizaje en Python e IA.
